{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6-3amvH8DdB"
      },
      "source": [
        "from absl import logging\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import csv\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "!pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clZeQAsxIs2J"
      },
      "source": [
        "Load the data from link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn1w-ngk8Ixy"
      },
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyu03J1WIvLI"
      },
      "source": [
        "Unzip the folder of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBLLFhph8O36"
      },
      "source": [
        "!tar --gunzip --extract --verbose --file=MovieSummaries.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwc7wIM183zY"
      },
      "source": [
        "%cd /content/MovieSummaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_AXSCXm87rF"
      },
      "source": [
        "!ls -lh  *.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRfSy_qDI2Qo"
      },
      "source": [
        "here we will open that data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMMblfvN8-dk"
      },
      "source": [
        "meta = pd.read_csv(\"movie.metadata.tsv\", sep = '\\t', header = None)\n",
        "meta.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_tB4agiI9TW"
      },
      "source": [
        "generate names of columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbxHnFMX9BjV"
      },
      "source": [
        "# rename columns\n",
        "meta.columns = [\"movie_id\",1,\"movie_name\",3,4,5,6,7,\"genre\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W04kqVWJBSc"
      },
      "source": [
        "upload the dataset of movieplots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Ud-_-M9E-7"
      },
      "source": [
        "plots = []\n",
        "\n",
        "with open(\"plot_summaries.txt\", 'r') as f:\n",
        "       reader = csv.reader(f, dialect='excel-tab') \n",
        "       for row in tqdm(reader):\n",
        "            plots.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpxfRFObJG3c"
      },
      "source": [
        "we will split up movie plots and data id into two different list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "997jWRd69U2I"
      },
      "source": [
        "movie_id = []\n",
        "plot = []\n",
        "\n",
        "# extract movie Ids and plot summaries\n",
        "for i in tqdm(plots):\n",
        "  movie_id.append(i[0])\n",
        "  plot.append(i[1])\n",
        "\n",
        "# create dataframe\n",
        "movies = pd.DataFrame({'movie_id': movie_id, 'plot': plot})\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcD0TpWHJZ09"
      },
      "source": [
        "here we will add names of movies and thier generes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oosaFmz_9drG"
      },
      "source": [
        "# change datatype of 'movie_id'\n",
        "meta['movie_id'] = meta['movie_id'].astype(str)\n",
        "\n",
        "# merge meta with movies\n",
        "movies = pd.merge(movies, meta[['movie_id', 'movie_name', 'genre']], on = 'movie_id')\n",
        "\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSztVCn-EjA"
      },
      "source": [
        "type(json.loads(movies['genre'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJMVESQQJfS4"
      },
      "source": [
        "extract all the generes from movie data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k496LFpc-Ke8"
      },
      "source": [
        "# an empty list\n",
        "genres = [] \n",
        "\n",
        "# extract genres\n",
        "for i in movies['genre']: \n",
        "  genres.append(list(json.loads(i).values())) \n",
        "\n",
        "# add to 'movies' dataframe  \n",
        "movies['genre_new'] = genres"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wne18gPG-Nlr"
      },
      "source": [
        "# remove samples with 0 genre tags\n",
        "movies_new = movies[~(movies['genre_new'].str.len() == 0)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4XTzetr-QpD"
      },
      "source": [
        "movies_new.shape, movies.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X4youD--Tn0"
      },
      "source": [
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeNNKkEoLEnK"
      },
      "source": [
        "count the movie generes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxfHX-Kd-XFl"
      },
      "source": [
        "# get all genre tags in a list\n",
        "all_genres = sum(genres,[])\n",
        "len(set(all_genres))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LVghejJoAZ"
      },
      "source": [
        "Here we will create a dictionary of genres and their occurrence count across the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4b8G3w-ahZ"
      },
      "source": [
        "all_genres = nltk.FreqDist(all_genres) \n",
        "\n",
        "# create dataframe\n",
        "all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), \n",
        "                              'Count': list(all_genres.values())})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9Pe1kyJsSb"
      },
      "source": [
        "clean the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB42zOiB-he4"
      },
      "source": [
        "# function for text cleaning \n",
        "def clean_text(text):\n",
        "    # remove backslash-apostrophe \n",
        "    text = re.sub(\"\\'\", \"\", text) \n",
        "    # remove everything except alphabets \n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
        "    # remove whitespaces \n",
        "    text = ' '.join(text.split()) \n",
        "    # convert text to lowercase \n",
        "    text = text.lower() \n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR-U7TOw-ohL"
      },
      "source": [
        "pd.set_option('mode.chained_assignment', None)\n",
        "movies_new['clean_plot'] = movies_new['plot'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls4nBo2dJyR8"
      },
      "source": [
        "Below we will remove stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B_yC10l-0DG"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zibvvPq-8oz"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# function to remove stopwords\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "movies_new['clean_plot'] = movies_new['clean_plot'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80vXt3OA58Ju"
      },
      "source": [
        "here we are going to be new target varibles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMPTqb1o-_6w"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(movies_new['genre_new'])\n",
        "\n",
        "# transform target variable\n",
        "y = multilabel_binarizer.transform(movies_new['genre_new'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQj6hFVh_N_E"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1uGVZQiJ7ju"
      },
      "source": [
        "we will split our data into training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVV7f07_Q1S"
      },
      "source": [
        "# split dataset into training and validation set\n",
        "xtrain, xval, ytrain, yval = train_test_split(movies_new['clean_plot'], y, test_size=0.2, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-aDRB3_UcZ"
      },
      "source": [
        "# create TF-IDF features\n",
        "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
        "xval_tfidf = tfidf_vectorizer.transform(xval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhrkRtFPKdPx"
      },
      "source": [
        "Here we will build a Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9pnL6Ee_XNM"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Binary Relevance\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Performance metric\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ2qt4ZG_cug"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "clf = OneVsRestClassifier(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E25_6ij1_fyJ"
      },
      "source": [
        "# fit model on train data\n",
        "clf.fit(xtrain_tfidf, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ke0gibM_iRs"
      },
      "source": [
        "# make predictions for validation set\n",
        "y_pred = clf.predict(xval_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcDjOGj7ASDZ"
      },
      "source": [
        "# evaluate performance\n",
        "f1_score(yval, y_pred, average=\"micro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrgGopyPej5B"
      },
      "source": [
        "# predict probabilities\n",
        "y_pred_prob = clf.predict_proba(xval_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbPgWBeFehey"
      },
      "source": [
        "t = 0.3 # threshold value\n",
        "y_pred_new = (y_pred_prob >= t).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcDkCv1betEs"
      },
      "source": [
        "# evaluate performance\n",
        "f1_score(yval, y_pred_new, average=\"micro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quZsAc23ewzU"
      },
      "source": [
        "build an inference function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvyqz_mFAeZK"
      },
      "source": [
        "def infer_tags(q):\n",
        "    q = clean_text(q)\n",
        "    q = remove_stopwords(q)\n",
        "    q_vec = tfidf_vectorizer.transform([q])\n",
        "    q_pred = clf.predict(q_vec)\n",
        "    return multilabel_binarizer.inverse_transform(q_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gus0jFXXKL45"
      },
      "source": [
        "Final text classification!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLusODbbAh6y"
      },
      "source": [
        "for i in range(5): \n",
        "  k = xval.sample(1).index[0] \n",
        "  print(\"Movie: \", movies_new['movie_name'][k], \"\\nPredicted genre: \", infer_tags(xval[k])), print(\"Actual genre: \",movies_new['genre_new'][k], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPBNJIFRMfv1"
      },
      "source": [
        "Load all files from a directory in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAIlcPQcMhr0"
      },
      "source": [
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "train_df, test_df = download_and_load_datasets()\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEOAlICNMvRp"
      },
      "source": [
        "Training input on the whole training set with no limit on training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68PYiZDwMrJI"
      },
      "source": [
        "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\n",
        "\n",
        "# Prediction on the whole training set.\n",
        "predict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    train_df, train_df[\"polarity\"], shuffle=False)\n",
        "# Prediction on the test set.\n",
        "predict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    test_df, test_df[\"polarity\"], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8ScO0FoNJnI"
      },
      "source": [
        "Here we will be using feature column that applies a module on the given text feature and passes further the outputs of the module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqQv_EWLNBBl"
      },
      "source": [
        "embedded_text_feature_column = hub.text_embedding_column(\n",
        "    key=\"sentence\", \n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNYq770hNS0A"
      },
      "source": [
        "For classification we can use a DNN Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTXkAOI3NOi3"
      },
      "source": [
        "estimator = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[500, 100],\n",
        "    feature_columns=[embedded_text_feature_column],\n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEpKW9gqNZaE"
      },
      "source": [
        "Train the estimator for a reasonable amount of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2YoMeSxNWPy"
      },
      "source": [
        "# Training for 5,000 steps means 640,000 training examples with the default\n",
        "# batch size. This is roughly equivalent to 25 epochs since the training dataset\n",
        "# contains 25,000 examples.\n",
        "estimator.train(input_fn=train_input_fn, steps=5000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1I1NkLrNfns"
      },
      "source": [
        "Run predictions for both training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9opK0oDXNcw3"
      },
      "source": [
        "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
        "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmM9226nNnsn"
      },
      "source": [
        "We can visually check the confusion matrix to understand the distribution of misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkkGSzqgNjw0"
      },
      "source": [
        "def get_predictions(estimator, input_fn):\n",
        "  return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
        "\n",
        "LABELS = [\n",
        "    \"negative\", \"positive\"\n",
        "]\n",
        "\n",
        "# Create a confusion matrix on training data.\n",
        "cm = tf.math.confusion_matrix(train_df[\"polarity\"], \n",
        "                              get_predictions(estimator, predict_train_input_fn))\n",
        "\n",
        "# Normalize the confusion matrix so that each row sums to 1.\n",
        "cm = tf.cast(cm, dtype=tf.float32)\n",
        "cm = cm / tf.math.reduce_sum(cm, axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS);\n",
        "plt.xlabel(\"Predicted\");\n",
        "plt.ylabel(\"True\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fd1WKcbN4cr"
      },
      "source": [
        "here we will run a couple of trainings and evaluations to see how using a various modules can affect the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8LjVqajNsEN"
      },
      "source": [
        "def train_and_evaluate_with_module(hub_module, train_module=False):\n",
        "  embedded_text_feature_column = hub.text_embedding_column(\n",
        "      key=\"sentence\", module_spec=hub_module, trainable=train_module)\n",
        "\n",
        "  estimator = tf.estimator.DNNClassifier(\n",
        "      hidden_units=[500, 100],\n",
        "      feature_columns=[embedded_text_feature_column],\n",
        "      n_classes=2,\n",
        "      optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.003))\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, steps=1000)\n",
        "\n",
        "  train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
        "  test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
        "\n",
        "  training_set_accuracy = train_eval_result[\"accuracy\"]\n",
        "  test_set_accuracy = test_eval_result[\"accuracy\"]\n",
        "\n",
        "  return {\n",
        "      \"Training accuracy\": training_set_accuracy,\n",
        "      \"Test accuracy\": test_set_accuracy\n",
        "  }\n",
        "\n",
        "\n",
        "results = {}\n",
        "results[\"nnlm-en-dim128\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
        "results[\"nnlm-en-dim128-with-module-training\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/nnlm-en-dim128/1\", True)\n",
        "results[\"random-nnlm-en-dim128\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/random-nnlm-en-dim128/1\")\n",
        "results[\"random-nnlm-en-dim128-with-module-training\"] = train_and_evaluate_with_module(\n",
        "    \"https://tfhub.dev/google/random-nnlm-en-dim128/1\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWHqp-GuOAk7"
      },
      "source": [
        "RESULTS!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tSBXQpTN9h8"
      },
      "source": [
        "pd.DataFrame.from_dict(results, orient=\"index\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQsMxnBOKfj"
      },
      "source": [
        "We should establish the baseline accuracy of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnXUaWoMOD-b"
      },
      "source": [
        "estimator.evaluate(input_fn=predict_test_input_fn)[\"accuracy_baseline\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaOMQnBORT-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}